Mateusz BiegaÅ„ski
mb385162

SAD 2020-2021

## Ex. 1

```{r}
load("data/cancer.RData")
```

```{r}
library("dplyr")
```

```{r}
dim(data.test)
dim(data.train)
```

```{r}
data.train %>% dplyr::select(where(is.numeric)) %>% dim
```

As we see, all of 17 737 predictors are **quantitive** variables.

```{r}
dim(data.train)
predictors = data.train[,!(names(data.train) == "Y")]
Y = data.train[,(names(data.train) == "Y")]
```

```{r}
library("Rfast")
variances <- colVars(as.matrix(predictors))
variances <- cbind(colnames(predictors) %>% data.frame, as.numeric(variances))

variances.top <- variances[order(variances[,2], decreasing=TRUE) ,] %>% head(n=500)
head(variances.top)
```

```{r}
predictors.top <- predictors[,(colnames(predictors) %in% variances.top[,1])]
predictors.top %>% head
predictors.top.cor <- predictors.top %>% cor
predictors.top.cor %>% head
```

```{r}
sapply(predictors.top.cor, mean) %>% hist
```

Let's look at the correlation plot of 10 (not 500 for visibility purposes) arbitrary-selected variables.

```{r}
library(ggplot2)

num=10
plot_df <- reshape2::melt(predictors.top.cor[1:num, 1:num])
ggplot(data = plot_df) + geom_violin(aes(Var1, value, fill = Var1))
```

## Ex. 2

### Elastic net

to standard linear regression model penalty there is an additional factor added:

$\lambda$*($\alpha$* $\lVert\beta\rVert _1$ + (1-$\alpha$)*$\lVert\beta\rVert _2 ^2$ )

It is sort of mix of Lasso and Ridge regression model - it can easily fix highly correlated variables issue, by clustering and either emphasizing or suppressing whole groups.

* Estimated parameters:
    * $\lambda$
* Tuning parameters:
    * $\alpha$

## Ex. 3

Let's observe, that variables with zero or near-zero variance aren't very meaningful for us - it's hard to explain response variable in terms of linear combination of near-constant variables. Instead let's focus on topN (N=500) variables, in terms of variance.

## Ex. 4

```{r}
elastic_fn = function(alpha) {
    cvfit <- cv.glmnet(predictors.top %>% as.matrix, Y %>% as.matrix, alpha=alpha, type.measure="mse")
    plot(cvfit)
    print(cvfit)
    cvfit
}
```

```{r}
library("purrr")
tuning_alpha = c(0.0, .5, .6, .7, .98, 1.0)

par(mfrow=c(2,3))
cvfits <- map(tuning_alpha, elastic_fn)
```

```{r}
par(mfrow=c(2,3))
map2(cvfits, indices_1se, function(fit, idx) {plot(fit$cvsd, ylim=c(0, 0.005)) }) %>% invisible
cvms <- map2(cvfits, indices_1se, function(fit, idx) { fit$cvsd[[idx]] })
cvms
```

| $\alpha$ | 1se $\lambda$ | 1se MSE   |  st. err  | non-zero |
|----------|---------------|-----------|-----------|----------|
|    0.0   |      1.715    |   0.049   |    0.0030 |   500    |
|    0.5   |      0.046    |   0.051   |    0.0030 |    32    |
|    0.6   |      0.035    |   0.050   |    0.0020 |    34    |
|    0.7   |      0.030    |   0.050   |    0.0027 |    33    |
|   0.98   |      0.022    |   0.051   |    0.0030 |    33    |
|   1.00   |      0.021    |   0.050   |    0.0027 |    33    |
    

 The results may be a bit different from run to run, as cv.glmnet introduces some randomness.

As we see resuts for $\alpha \ge 0.5$ are quite similar, however standard error estimate for  $\alpha = 0.6$ is outstandingly low (as you can see in plot above). For that reason I decided to pick that model and train it on whole data, as specified in `Build final model` section.

Elastic net on our data yields predictions with quality ~95% (using only $\leq$ 35 parameters). 

### Random forest

```{r}
library("randomForest")
rv.fit <- rfcv(predictors.top, Y, cv.fold=10)
```

```{r}
with(rv.fit, print(error.cv))
```

```{r}
with(rv.fit, error.cv %>% plot)
```

As we see, random forest cross validation yields very similar results to Elastic net.

## Building final model

To avoid overfitting, I pick 1se error instead of lambda.min.

```{r}
data.test.top <- data.test[, (names(data.test) %in% colnames(predictors.top))]
dim(data.test.top)
head(data.test.top, n=3)
```

```{r}
glmnet(data.test.top,)


coef_cv=coef(cvfit, s = "lambda.min")
# prediction of the final model
predict(cvfit, newx = x[1:5,], s = "lambda.min")
```

```{r}
# make sure that order of test columns corresponds to train columns
all(predictors.top %>% names == data.test.top %>% names)
```

```{r}
model = cvfits[[which(tuning_alpha == .6)]]

pred <- predict(model, data.test.top %>% as.matrix) 
dim(pred)
head(pred)
```

```{r}
fname = "bieganski.RData"
save(pred, file = fname)
pred = "DUMMY" # to check whether it loads properly
load(fname)
pred %>% head
```